# Mod√®les IA par Type d'Op√©ration

**Date** : 12 f√©vrier 2026
**Version** : 1.0

---

## üìä Vue d'Ensemble

Ce document d√©taille **exactement quels mod√®les LLM et embeddings** sont utilis√©s pour chaque op√©ration dans Qadhya.

---

## üéØ R√©capitulatif par Op√©ration

### 1Ô∏è‚É£ Indexation (Background Processing)

**Route** : `/api/admin/index-kb` (cron)
**Op√©rationName** : `indexation`

| Composant | Provider | Mod√®le | Dimensions | Co√ªt |
|-----------|----------|--------|------------|------|
| **Embeddings** | Ollama | `qwen3-embedding:0.6b` | 1024 | 0‚Ç¨ |
| **LLM Classification** | Ollama | `qwen2.5:3b` | - | 0‚Ç¨ |
| **Fallback LLM** | Groq ‚Üí Gemini ‚Üí DeepSeek | Voir strat√©gie d√©faut | - | 0‚Ç¨ (si √©chec Ollama) |

**Strat√©gie** :
- Ollama embeddings **exclusif** (√©conomie maximale, volume √©lev√©)
- Ollama LLM pour classification (gratuit, illimit√©)
- Fallback cloud uniquement si Ollama down

**Timeout** : 60s total
**Temperature** : 0.2 (d√©terministe pour classification)

---

### 2Ô∏è‚É£ Assistant IA (Chat Temps R√©el)

**Route** : `/api/chat`
**Op√©rationName** : `assistant-ia`

| Composant | Provider | Mod√®le | Dimensions | Latence | Co√ªt |
|-----------|----------|--------|------------|---------|------|
| **Embeddings** | Ollama | `qwen3-embedding:0.6b` | 1024 | ~3-5s | 0‚Ç¨ |
| **LLM Primaire** | Groq | `llama-3.3-70b-versatile` | - | **292ms** | 0‚Ç¨ |
| **LLM Fallback 1** | Gemini | `gemini-2.0-flash-exp` | - | ~1.5s | 0‚Ç¨ |
| **LLM Fallback 2** | DeepSeek | `deepseek-chat` | - | ~1.8s | ~$0.14/1M tokens |
| **LLM Fallback 3** | Ollama | `qwen2.5:3b` | - | ~15-20s | 0‚Ç¨ |

**Strat√©gie LLM** :
1. **Groq llama-3.3-70b** (ultra-rapide, gratuit) ‚ö°
2. Si √©chec/rate limit ‚Üí **Gemini 2.0 Flash** (rapide, contexte 1M tokens)
3. Si √©chec ‚Üí **DeepSeek** (√©conomique, qualit√© correcte)
4. Si √©chec ‚Üí **Ollama qwen2.5:3b** (local, lent mais gratuit)

**Timeout** : 10s total
**Temperature** : 0.3 (conversationnel naturel)
**Format** : Chat conversationnel

**Co√ªt estim√©** : 0‚Ç¨/mois (Groq gratuit pour 95%+ des requ√™tes)

---

### 3Ô∏è‚É£ Assistant Dossiers (Analyse Approfondie)

**Route** : `/api/dossiers/[id]/assistant`
**Op√©rationName** : `dossiers-assistant`

| Composant | Provider | Mod√®le | Dimensions | Latence | Co√ªt |
|-----------|----------|--------|------------|---------|------|
| **Embeddings** | OpenAI | `text-embedding-3-small` | **1536** | ~0.5-1s | ~$0.02/1M tokens |
| **Embeddings Fallback** | Ollama | `qwen3-embedding:0.6b` | 1024 | ~3-5s | 0‚Ç¨ |
| **LLM Primaire** | Gemini | `gemini-2.0-flash-exp` | - | ~1.5-3s | 0‚Ç¨ |
| **LLM Fallback 1** | Groq | `llama-3.3-70b-versatile` | - | ~292ms | 0‚Ç¨ |
| **LLM Fallback 2** | DeepSeek | `deepseek-chat` | - | ~1.8s | ~$0.14/1M tokens |

**Strat√©gie LLM** :
1. **Gemini 2.0 Flash** (contexte 1M tokens, qualit√© + vitesse)
2. Si √©chec ‚Üí **Groq llama-3.3-70b** (ultra-rapide)
3. Si √©chec ‚Üí **DeepSeek** (√©conomique)

**Strat√©gie Embeddings** :
- **OpenAI text-embedding-3-small** (1536-dim) pour qualit√© maximale
- Fallback Ollama si OpenAI indisponible

**Timeout** : 30s total
**Temperature** : 0.2 (pr√©cis et factuel)
**Format** : Chat conversationnel mais approfondi

**Co√ªt estim√©** : ~1-2‚Ç¨/mois (OpenAI embeddings uniquement, volume faible)

---

### 4Ô∏è‚É£ Consultation Juridique Formelle (IRAC)

**Route** : `/api/dossiers/[id]/consultation`
**Op√©rationName** : `dossiers-consultation`

| Composant | Provider | Mod√®le | Dimensions | Latence | Co√ªt |
|-----------|----------|--------|------------|---------|------|
| **Embeddings** | OpenAI | `text-embedding-3-small` | **1536** | ~0.5-1s | ~$0.02/1M tokens |
| **Embeddings Fallback** | Ollama | `qwen3-embedding:0.6b` | 1024 | ~3-5s | 0‚Ç¨ |
| **LLM Primaire** | Gemini | `gemini-2.0-flash-exp` | - | ~3-10s | 0‚Ç¨ |
| **LLM Fallback 1** | DeepSeek | `deepseek-chat` | - | ~1.8s | ~$0.14/1M tokens |
| **LLM Fallback 2** | Groq | `llama-3.3-70b-versatile` | - | ~292ms | 0‚Ç¨ |

**Strat√©gie LLM** :
1. **Gemini 2.0 Flash** (contexte 1M tokens, raisonnement approfondi)
2. Si √©chec ‚Üí **DeepSeek** (√©conomique, bonne qualit√©)
3. Si √©chec ‚Üí **Groq llama-3.3-70b** (rapide)

**Strat√©gie Embeddings** :
- **OpenAI text-embedding-3-small** (1536-dim) pour qualit√© maximale

**Timeout** : 60s total (consultation d√©taill√©e)
**Temperature** : 0.1 (tr√®s factuel et pr√©cis)
**Format** : **IRAC** (Issue, Rule, Application, Conclusion)

**Prompt Type** : `consultation` (voir `lib/ai/legal-reasoning-prompts.ts`)

**Co√ªt estim√©** : ~1-2‚Ç¨/mois (OpenAI embeddings uniquement, volume tr√®s faible)

---

## üìà Tableau R√©capitulatif Global

| Op√©ration | LLM Principal | LLM Mod√®le | Embeddings Provider | Embeddings Mod√®le | Dim | Timeout | Temp | Co√ªt/mois |
|-----------|--------------|------------|-------------------|------------------|-----|---------|------|-----------|
| **Indexation** | Ollama | qwen2.5:3b | Ollama | qwen3-embedding:0.6b | 1024 | 60s | 0.2 | 0‚Ç¨ |
| **Assistant IA** | Groq | llama-3.3-70b | Ollama | qwen3-embedding:0.6b | 1024 | 10s | 0.3 | 0‚Ç¨ |
| **Dossiers Assistant** | Gemini | gemini-2.0-flash | OpenAI | text-embedding-3-small | 1536 | 30s | 0.2 | ~2‚Ç¨ |
| **Consultation** | Gemini | gemini-2.0-flash | OpenAI | text-embedding-3-small | 1536 | 60s | 0.1 | ~2‚Ç¨ |

**Total estim√©** : **~4-6‚Ç¨/mois** (vs ~100‚Ç¨/mois avant) = **-95% √©conomies** üéâ

---

## üîß Configuration Source

Tous ces mod√®les sont configur√©s dans **`lib/ai/operations-config.ts`**.

Pour changer un mod√®le, modifier ce fichier puis tester avec :
```bash
npm run test:operations-config
```

---

## üéØ Pourquoi ces Choix ?

### Indexation
- **Volume √©lev√©** (milliers d'embeddings/jour) ‚Üí Ollama gratuit uniquement
- **Co√ªt critique** ‚Üí 0‚Ç¨ obligatoire

### Assistant IA
- **Latence critique** (< 3s attendu) ‚Üí Groq ultra-rapide (292ms)
- **Volume √©lev√©** ‚Üí Gratuit requis (Groq tier gratuit)
- **Embeddings** ‚Üí Ollama (volume √©lev√©, cache)

### Dossiers Assistant
- **Qualit√© critique** ‚Üí OpenAI embeddings 1536-dim
- **Contexte √©tendu** ‚Üí Gemini 1M tokens
- **Volume faible** (~10-50 analyses/mois) ‚Üí Co√ªt OpenAI acceptable

### Consultation
- **Pr√©cision maximale** ‚Üí Temperature 0.1 + OpenAI embeddings
- **Format structur√©** ‚Üí Gemini excellent pour IRAC
- **Volume tr√®s faible** (~5-20 consultations/mois) ‚Üí Co√ªt acceptable

---

## üìä D√©tails Techniques par Provider

### Groq (llama-3.3-70b-versatile)
- **Vitesse** : 292ms moyenne (ultra-rapide)
- **Contexte** : 8K tokens
- **Co√ªt** : Gratuit (tier gratuit g√©n√©reux)
- **Qualit√©** : Excellente pour chat conversationnel
- **Usage** : Assistant IA (95%+ des requ√™tes)

### Gemini (gemini-2.0-flash-exp)
- **Vitesse** : 1.5-3s (rapide)
- **Contexte** : 1M tokens (√©norme)
- **Co√ªt** : Gratuit (tier gratuit)
- **Qualit√©** : Excellente pour raisonnement
- **Usage** : Dossiers + Consultations (primaire)

### DeepSeek (deepseek-chat)
- **Vitesse** : 1.8s (correct)
- **Contexte** : 128K tokens
- **Co√ªt** : ~$0.14/1M tokens (√©conomique)
- **Qualit√©** : Bonne
- **Usage** : Fallback dossiers/consultations

### Ollama (qwen2.5:3b + qwen3-embedding:0.6b)
- **Vitesse** : 15-20s chat, 3-5s embeddings (lent, CPU-only)
- **Contexte** : 128K tokens chat
- **Co√ªt** : 0‚Ç¨ (local)
- **Qualit√©** : Correcte pour classification, embeddings
- **Usage** : Indexation (primaire) + fallback chat

### OpenAI (text-embedding-3-small)
- **Vitesse** : 0.5-1s (tr√®s rapide)
- **Dimensions** : 1536 (vs 1024 Ollama)
- **Co√ªt** : ~$0.02/1M tokens
- **Qualit√©** : Excellente (meilleure que Ollama)
- **Usage** : Dossiers + Consultations uniquement

---

## üîç Comment V√©rifier le Mod√®le Utilis√© ?

### Dans les Logs

```bash
# Chat utilisateur
[LLM-Fallback] Op√©ration: assistant-ia ‚Üí Strat√©gie: [groq ‚Üí gemini ‚Üí deepseek ‚Üí ollama]
[LLM-Fallback] ‚úì groq llama-3.3-70b-versatile (292ms)

# Dossiers
[LLM-Fallback] Op√©ration: dossiers-assistant ‚Üí Strat√©gie: [gemini ‚Üí groq ‚Üí deepseek]
[Embeddings] OpenAI text-embedding-3-small (1536-dim)
[LLM-Fallback] ‚úì gemini gemini-2.0-flash-exp (1.8s)
```

### Dans la R√©ponse API

```json
{
  "answer": "...",
  "model": "groq/llama-3.3-70b-versatile",
  "tokensUsed": { "input": 120, "output": 350, "total": 470 }
}
```

---

## üìö R√©f√©rences

- **Configuration** : `lib/ai/operations-config.ts`
- **LLM Service** : `lib/ai/llm-fallback-service.ts`
- **Embeddings Service** : `lib/ai/embeddings-service.ts`
- **Config AI globale** : `lib/ai/config.ts`
- **Prompts IRAC** : `lib/ai/legal-reasoning-prompts.ts`

---

**Derni√®re mise √† jour** : 12 f√©vrier 2026
