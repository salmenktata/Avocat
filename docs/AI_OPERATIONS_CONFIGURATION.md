# Configuration IA par Type d'Op√©ration

**Date de cr√©ation** : 12 f√©vrier 2026
**Version** : 1.0
**Statut** : ‚úÖ Impl√©ment√©

---

## Vue d'ensemble

Le syst√®me Qadhya impl√©mente maintenant une configuration IA **sp√©cifique par type d'op√©ration m√©tier**. Chaque op√©ration (indexation, chat utilisateur, analyse dossier, consultation) dispose de sa propre configuration optimis√©e en termes de :

- üéØ **Providers LLM** (Groq, Gemini, DeepSeek, Ollama)
- üìä **Providers embeddings** (Ollama, OpenAI)
- ‚è±Ô∏è **Timeouts** adapt√©s
- üß† **Param√®tres LLM** (temp√©rature, maxTokens)
- üí∞ **Optimisation co√ªt/performance**

---

## Architecture

### Fichiers Core

| Fichier | R√¥le |
|---------|------|
| `lib/ai/operations-config.ts` | **Configuration centralis√©e** par op√©ration |
| `lib/ai/llm-fallback-service.ts` | Support `operationName` dans `LLMOptions` |
| `lib/ai/embeddings-service.ts` | Support `operationName` dans `EmbeddingOptions` |
| `lib/ai/rag-chat-service.ts` | Support `operationName` dans `ChatOptions` |
| `app/api/chat/route.ts` | Utilise `operationName: 'assistant-ia'` |

### Types d'Op√©rations

```typescript
type OperationName =
  | 'indexation'              // Indexation KB background
  | 'assistant-ia'            // Chat utilisateur temps r√©el
  | 'dossiers-assistant'      // Analyse approfondie dossier
  | 'dossiers-consultation'   // Consultation juridique formelle
```

---

## Configuration par Op√©ration

### 1. Indexation (Background Processing)

**Use Case** : Indexation KB en arri√®re-plan (volume √©lev√©, co√ªt critique)

```typescript
'indexation': {
  context: 'embeddings',  // Utilise strat√©gie embeddings existante

  embeddings: {
    provider: 'ollama',  // Gratuit pour volume √©lev√©
    fallbackProvider: 'openai',
    model: 'qwen3-embedding:0.6b',
    dimensions: 1024,
  },

  timeouts: {
    embedding: 10000,  // 10s par embedding
    chat: 30000,       // 30s pour classification LLM
    total: 60000,      // 1min total max
  },

  llmConfig: {
    temperature: 0.2,  // D√©terministe pour classification
    maxTokens: 2000,
  },
}
```

**Objectifs** :
- ‚úÖ Co√ªt 0‚Ç¨ (Ollama exclusif)
- ‚úÖ Volume √©lev√© support√© (5-10M tokens/jour)
- ‚úÖ Fallback OpenAI si Ollama indisponible

---

### 2. Assistant IA (Chat Temps R√©el)

**Use Case** : Chat utilisateur temps r√©el (performance critique, volume √©lev√©)

```typescript
'assistant-ia': {
  context: 'rag-chat',

  providers: {
    primary: 'groq',  // Ultra-rapide (292ms)
    fallback: ['gemini', 'deepseek', 'ollama'],
  },

  embeddings: {
    provider: 'ollama',  // Gratuit pour volume √©lev√©
    model: 'qwen3-embedding:0.6b',
    dimensions: 1024,
  },

  timeouts: {
    embedding: 3000,   // 3s max (cache attendu)
    chat: 5000,        // 5s max (Groq ultra-rapide)
    total: 10000,      // 10s total
  },

  llmConfig: {
    temperature: 0.3,  // Conversationnel naturel
    maxTokens: 500,    // R√©ponses concises
    systemPromptType: 'chat',
  },
}
```

**Objectifs** :
- ‚ö° Latence < 3s (95% des cas)
- üí∞ Co√ªt ~0‚Ç¨ (Groq gratuit + Ollama embeddings)
- üéØ Qualit√© conversationnelle

---

### 3. Assistant Dossiers (Analyse Approfondie)

**Use Case** : Analyse approfondie dossier (qualit√© critique)

```typescript
'dossiers-assistant': {
  context: 'quality-analysis',

  providers: {
    primary: 'gemini',  // Qualit√© + contexte 1M tokens
    fallback: ['groq', 'deepseek'],
  },

  embeddings: {
    provider: 'openai',   // Qualit√© sup√©rieure (1536-dim)
    fallbackProvider: 'ollama',
    model: 'text-embedding-3-small',
    dimensions: 1536,
  },

  timeouts: {
    embedding: 5000,   // 5s max
    chat: 15000,       // 15s (analyse approfondie OK)
    total: 30000,      // 30s total
  },

  llmConfig: {
    temperature: 0.2,  // Pr√©cis et factuel
    maxTokens: 2000,   // R√©ponses d√©taill√©es
    systemPromptType: 'chat',
  },
}
```

**Objectifs** :
- üß† Qualit√© maximale (embeddings OpenAI 1536-dim)
- üìä Contexte √©tendu (Gemini 1M tokens)
- üí∞ Co√ªt mod√©r√© (~2‚Ç¨/mois estim√©)

---

### 4. Consultation (G√©n√©ration Formelle IRAC)

**Use Case** : Consultation juridique formelle (qualit√© maximale)

```typescript
'dossiers-consultation': {
  context: 'structuring',

  providers: {
    primary: 'gemini',  // Qualit√© + raisonnement
    fallback: ['deepseek', 'groq'],
  },

  embeddings: {
    provider: 'openai',   // Qualit√© maximale
    fallbackProvider: 'ollama',
    model: 'text-embedding-3-small',
    dimensions: 1536,
  },

  timeouts: {
    embedding: 5000,   // 5s max
    chat: 30000,       // 30s (consultation d√©taill√©e)
    total: 60000,      // 1min total
  },

  llmConfig: {
    temperature: 0.1,  // Tr√®s factuel et pr√©cis
    maxTokens: 4000,   // Consultation longue
    systemPromptType: 'consultation',
  },
}
```

**Objectifs** :
- üìú Format IRAC strict
- üéì Raisonnement juridique approfondi
- üí∞ Co√ªt acceptable (~2‚Ç¨/mois estim√©)

---

## Utilisation

### Dans une Route API

```typescript
// app/api/chat/route.ts
const response = await answerQuestion(question, userId, {
  dossierId,
  conversationId,
  includeJurisprudence,
  usePremiumModel,
  operationName: 'assistant-ia', // ‚Üê Configuration optimis√©e
})
```

### Dans un Service

```typescript
// lib/ai/rag-chat-service.ts
const queryEmbedding = await generateEmbedding(question, {
  operationName: options.operationName, // ‚Üê Utilise config op√©ration
})

const llmResponse = await callLLMWithFallback(
  messages,
  {
    temperature,
    maxTokens,
    systemPrompt,
    operationName: options.operationName, // ‚Üê Utilise config op√©ration
  },
  usePremiumModel
)
```

---

## Avantages

### 1. Performance Optimis√©e

- **Chat utilisateur** : Groq ultra-rapide (292ms)
- **Indexation** : Ollama gratuit (0‚Ç¨)
- **Dossiers** : Gemini contexte 1M tokens

### 2. Co√ªt Optimis√©

| Op√©ration | Provider Embeddings | Provider LLM | Co√ªt Estim√© |
|-----------|-------------------|--------------|-------------|
| Indexation | Ollama | Groq/Gemini | 0‚Ç¨/mois |
| Assistant IA | Ollama | Groq | 0‚Ç¨/mois |
| Dossiers Assistant | OpenAI | Gemini | ~2‚Ç¨/mois |
| Consultation | OpenAI | Gemini | ~2‚Ç¨/mois |
| **TOTAL** | | | **~4-6‚Ç¨/mois** |

**√âconomies** : ~100‚Ç¨/mois ‚Üí ~6‚Ç¨/mois = **~1200‚Ç¨/an** üéâ

### 3. Qualit√© Adapt√©e

- **Chat** : Rapide et conversationnel (temp=0.3)
- **Consultation** : Factuel et pr√©cis (temp=0.1)
- **Dossiers** : Embeddings OpenAI 1536-dim (qualit√© maximale)

---

## Monitoring

### M√©triques par Op√©ration

```typescript
// √Ä impl√©menter (Phase 5 optionnelle)
await logAIMetrics({
  operationName: 'assistant-ia',
  provider: 'groq',
  latency: 292,
  tokensUsed: 450,
  success: true,
})
```

### Dashboard M√©triques

```sql
-- Latence moyenne par op√©ration
SELECT
  operation_name,
  AVG(latency_ms) as avg_latency,
  COUNT(*) as total_calls
FROM ai_metrics
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY operation_name;
```

---

## Tests

### Test Assistant IA

```bash
curl -X POST http://localhost:7002/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "Qu'est-ce qu'un contrat de travail?"}'

# V√©rifier :
# - Latence < 3s
# - Provider = groq
# - Embedding provider = ollama
```

### Test Indexation

```bash
# V√©rifier logs indexation
grep "operationName.*indexation" /var/log/qadhya/indexing.log

# V√©rifier provider = ollama (0‚Ç¨)
```

---

## Migration

### R√©trocompatibilit√©

‚úÖ **100% r√©trocompatible** : `operationName` est optionnel. Sans ce param√®tre, le comportement actuel est pr√©serv√© (utilise `context` par d√©faut).

```typescript
// ‚úÖ AVANT (toujours fonctionnel)
await answerQuestion(question, userId, {
  dossierId,
  conversationId,
})

// ‚úÖ APR√àS (optimis√©)
await answerQuestion(question, userId, {
  dossierId,
  conversationId,
  operationName: 'assistant-ia',
})
```

---

## Roadmap

### Phase 1 : Configuration Core ‚úÖ Impl√©ment√©

- [x] Cr√©er `operations-config.ts`
- [x] Adapter `llm-fallback-service.ts`
- [x] Adapter `embeddings-service.ts`
- [x] Adapter `rag-chat-service.ts`
- [x] Modifier `/api/chat` route

### Phase 2 : Routes Dossiers (√Ä venir)

- [ ] Cr√©er `/api/dossiers/[id]/assistant` route
- [ ] Cr√©er `/api/dossiers/[id]/consultation` route
- [ ] Impl√©menter g√©n√©ration consultation IRAC

### Phase 3 : Monitoring (Optionnel)

- [ ] Ajouter logging par op√©ration
- [ ] Dashboard m√©triques par op√©ration
- [ ] Alertes co√ªt/latence

---

## FAQ

### Pourquoi OpenAI pour les dossiers mais pas pour le chat ?

**Chat** : Volume √©lev√© (2-3M tokens/jour) ‚Üí Ollama gratuit
**Dossiers** : Volume faible (~10-50 ops/mois) + qualit√© critique ‚Üí OpenAI 1536-dim acceptable (~2‚Ç¨/mois)

### Comment forcer un provider sp√©cifique ?

Modifier `operations-config.ts` :

```typescript
'assistant-ia': {
  providers: {
    primary: 'gemini',  // ‚Üê Changer ici
    fallback: ['groq', 'deepseek'],
  },
}
```

### Puis-je cr√©er mes propres op√©rations ?

Oui ! Ajouter dans `operations-config.ts` :

```typescript
export type OperationName =
  | 'indexation'
  | 'assistant-ia'
  | 'mon-operation-custom'  // ‚Üê Nouvelle op√©ration

export const AI_OPERATIONS_CONFIG = {
  // ...
  'mon-operation-custom': {
    context: 'default',
    providers: { primary: 'groq', fallback: ['ollama'] },
    // ...
  },
}
```

---

## R√©f√©rences

- **Source unique de v√©rit√©** : `lib/ai/operations-config.ts`
- **Doc cl√©s IA** : `docs/API_KEYS_MANAGEMENT.md`
- **Strat√©gie IA** : `docs/AI_STRATEGY_AND_ORGANIZATION.md`
- **MEMORY.md** : Section "ü§ñ Option C - IA Hybride"

---

**Derni√®re mise √† jour** : 12 f√©vrier 2026
**Maintenu par** : √âquipe DevOps Qadhya
