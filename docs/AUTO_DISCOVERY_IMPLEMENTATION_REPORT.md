# Rapport d'Impl√©mentation : Auto-D√©couverte Intelligente de Liens

**Date** : 9 f√©vrier 2026
**Objectif** : Impl√©menter un syst√®me d'auto-d√©couverte de liens dynamiques pour augmenter la couverture du crawler web

---

## üéØ Objectif du Projet

Impl√©menter un syst√®me qui d√©couvre automatiquement les liens dynamiques cach√©s derri√®re des menus JavaScript sur les sites web, sans configuration manuelle. L'objectif est d'augmenter la couverture de crawl de **+525% sur IORT.tn** (8 ‚Üí 50-150 pages).

## ‚úÖ Ce qui a √©t√© Accompli

### 1. Architecture Compl√®te en 6 Phases

#### Phase 1 : Types et Interfaces
- **Fichier** : `lib/web-scraper/types.ts`
- Ajout de `'webdev'` au type `DetectedFramework`
- Nouveau champ `discoveredUrls?: string[]` dans `FetchResult`
- Nouvelle interface `LinkDiscoveryConfig` pour la configuration par framework

#### Phase 2 : Strat√©gies de Capture d'URLs
- **Fichier** : `lib/web-scraper/url-capture-strategies.ts` (nouveau)
- **4 strat√©gies impl√©ment√©es** :
  - `captureDomUrls()` : Parser les liens `<a href>` du DOM
  - `captureHistoryUrls()` : Espionner `pushState/replaceState` (SPAs)
  - `captureXhrUrls()` : Intercepter les requ√™tes XHR/Fetch
  - `captureHybridUrls()` : Combinaison des 3 (recommand√©)

#### Phase 3 : Service de D√©couverte de Menus
- **Fichier** : `lib/web-scraper/menu-discovery-service.ts` (nouveau)
- **Configurations par framework** : WebDev, Livewire, React, Vue, Angular, SPA g√©n√©rique
- **Scoring de pertinence** : Position dans DOM, texte pertinent, section (nav/footer)
- **Boucle de clics intelligente** : Conditions d'arr√™t (timeout, no new URLs, max clics)
- **Patterns d'exclusion** : logout, admin, login automatiquement exclus

#### Phase 4 : D√©tection Framework WebDev
- **Fichier** : `lib/web-scraper/scraper-service.ts`
- Profil WebDev ajout√© dans `FRAMEWORK_PROFILES`
- D√©tection via patterns : `WD_ACTION_`, `PAGE_Principal`, `gbWDInit`, `AWP_`
- Configuration optimis√©e : networkidle, 2.5s delay, 20s timeout

#### Phase 5 : Int√©gration dans le Scraper
- **Fichier** : `lib/web-scraper/scraper-service.ts`
- Phase de d√©couverte ins√©r√©e apr√®s scroll, avant `clickBeforeExtract`
- URLs d√©couvertes retourn√©es dans `FetchResult`
- Logs : `[Scraper] D√©couverte: X URLs (Y clics)`

#### Phase 6 : Propagation au Crawler
- **Fichier** : `lib/web-scraper/crawler-service.ts`
- URLs dynamiques ajout√©es √† la queue automatiquement
- D√©duplication via `hashUrl()`
- Log sp√©cifique : `üîó Lien dynamique ‚Üí URL`

### 2. Corrections de Bugs

#### Bug 1 : respect_robots_txt non respect√©
- **Probl√®me** : `getRobotsRules()` appel√© inconditionnellement
- **Impact** : Timeout sur sites avec robots.txt inaccessibles (IORT)
- **Solution** : V√©rification de `respect_robots_txt` avant appel
- **Commit** : `e5e87a6`

#### Bug 2 : Erreurs TypeScript dans les logs
- **Probl√®me** : Param√®tres implicites `any` dans les maps
- **Impact** : √âchec du build Docker dans GitHub Actions
- **Solution** : Typage explicite `(p: RegExp)`
- **Commit** : `07c4f9b`

### 3. Commits Cr√©√©s

```bash
31cfbc4 - feat: Impl√©menter auto-d√©couverte intelligente de liens via interaction JavaScript
e5e87a6 - fix: Respecter le param√®tre respect_robots_txt dans le crawler
07c4f9b - fix: Typage explicite pour les logs de debug du crawler
```

**Total** : +818 lignes de code, 7 fichiers modifi√©s, 2 fichiers cr√©√©s

### 4. D√©ploiement Production

- ‚úÖ Build Docker r√©ussi
- ‚úÖ Push vers `ghcr.io/salmenktata/moncabinet:latest`
- ‚úÖ Container red√©marr√© sur VPS
- ‚úÖ Code en production

## üìä Tests de Production

### Configuration Test√©e : IORT.tn

**Source** : Institut de l'Olivier de Tunisie
**URL** : https://www.iort.tn/siteiort/
**Framework** : WebDev (framework fran√ßais)

### R√©sultats des Tests

| M√©trique | R√©sultat |
|----------|----------|
| Pages d√©couvertes | 2 |
| Pages dynamiques | 1 (siteiort) |
| Pages WebDev d√©tect√©es | 1 (`/SITEIORT_WEB/L19/`) |
| Temps de crawl | <10s |

**URLs d√©couvertes** :
1. `https://www.iort.tn/siteiort/` (page d'accueil)
2. `https://www.iort.tn/SITEIORT_WEB/L19/ConfidentialiteMobile.awp` (page WebDev)

### √âtat vs Objectif

| Objectif | Attendu | Obtenu | √âcart |
|----------|---------|--------|-------|
| Pages totales | 50-150 | 2 | -96% |
| Pages dynamiques | 40+ | 1 | -97.5% |

## ‚ö†Ô∏è Probl√®mes Identifi√©s

### 1. Couverture Limit√©e

**Sympt√¥mes** :
- Seulement 2 pages d√©couvertes au lieu de 50-150
- Crawl se termine en quelques secondes
- Pas de logs de d√©couverte visible (`[MenuDiscovery]`, `[Scraper] D√©couverte`)

**Causes Possibles** :
1. **Framework non d√©tect√©** : WebDev peut ne pas √™tre reconnu sur la page d'accueil
2. **Timeout trop court** : Syst√®me ne laisse pas assez de temps pour la d√©couverte
3. **URL patterns restrictifs** : Certaines URLs dynamiques filtr√©es
4. **Job rest√© pending** : Job jamais trait√© par le worker

### 2. Probl√®mes de Configuration

**Trouv√©s** :
- Seed URL identique au base URL ‚Üí queue initiale avec doublons
- `respect_robots_txt=true` causait des timeouts
- CRON_SECRET diff√©rent entre local et production

## üéØ Prochaines √âtapes Recommand√©es

### Option A : Diagnostic Approfondi (prioritaire)

**Objectif** : Comprendre pourquoi la d√©couverte ne se d√©clenche pas

```bash
# 1. Observer les logs en temps r√©el
ssh root@84.247.165.187
docker logs -f moncabinet-nextjs | grep -i "framework\|d√©couverte\|webdev"

# 2. Lancer un crawl
curl -X GET "https://moncabinet.tn/api/cron/web-crawler" \
  -H "Authorization: Bearer <CRON_SECRET>"

# 3. V√©rifier la d√©tection du framework
# Chercher : "[Scraper] Frameworks d√©tect√©s: webdev"
```

**Points √† v√©rifier** :
- [ ] Le framework WebDev est-il d√©tect√© ?
- [ ] La fonction `discoverLinksViaInteraction()` est-elle appel√©e ?
- [ ] Des √©l√©ments cliquables sont-ils trouv√©s ?
- [ ] Des URLs sont-elles captur√©es ?

### Option B : Ajustement Configuration

```sql
UPDATE web_sources
SET
  max_depth = 5,
  max_pages = 1000,
  timeout_ms = 90000,
  rate_limit_ms = 3000,
  dynamic_config = jsonb_set(
    COALESCE(dynamic_config, '{}'::jsonb),
    '{linkDiscoveryConfig}',
    '{
      "enabled": true,
      "maxClicks": 30,
      "waitAfterClickMs": 2000,
      "discoveryTimeoutMs": 180000,
      "captureStrategy": "hybrid"
    }'::jsonb
  )
WHERE base_url LIKE '%iort%';
```

### Option C : Tests sur d'Autres Sites

**9anoun.tn (Livewire)** :
- Framework mieux support√©
- D√©j√† 100 pages d√©couvertes ‚Üí v√©rifier si le syst√®me d√©couvre plus
- Validation que le syst√®me fonctionne

**legislation.tn** :
- Autre site tunisien
- Validation de la robustesse

## üìà M√©triques de Succ√®s

### Crit√®res de Validation

- [x] Code d√©ploy√© en production
- [x] 0 erreurs TypeScript
- [x] Build Docker r√©ussi
- [x] Crawler fonctionnel
- [ ] Framework WebDev d√©tect√© automatiquement
- [ ] 50+ pages IORT d√©couvertes
- [ ] Logs de d√©couverte pr√©sents
- [ ] +50% couverture sur 9anoun.tn

### M√©triques Cibles

| Site | Pages Avant | Pages Cible | Gain Cible |
|------|-------------|-------------|------------|
| IORT.tn | 8 | 50-150 | +525-1775% |
| 9anoun.tn | 100 | 150-250 | +50-150% |
| legislation.tn | ~50 | ~75 | +50% |

## üèóÔ∏è Architecture du Syst√®me

```
fetchHtmlDynamic()
  ‚Üì
detectFramework() ‚Üí 'webdev', 'livewire', etc.
  ‚Üì
scroll() ‚Üí lazy loading
  ‚Üì
discoverLinksViaInteraction() ‚Üê NOUVEAU
  ‚îÇ
  ‚îú‚îÄ R√©cup√©rer config framework (MENU_DISCOVERY_CONFIGS)
  ‚îú‚îÄ D√©tecter √©l√©ments cliquables (s√©lecteurs CSS)
  ‚îú‚îÄ Scorer les √©l√©ments (pertinence)
  ‚îú‚îÄ Boucle de clics
  ‚îÇ   ‚îú‚îÄ Cliquer sur √©l√©ment
  ‚îÇ   ‚îú‚îÄ Attendre stabilisation
  ‚îÇ   ‚îî‚îÄ Capturer URLs (DOM/History/XHR/Hybrid)
  ‚îî‚îÄ Retourner discoveredUrls[]
  ‚Üì
clickBeforeExtract() ‚Üí legacy feature
  ‚Üì
extractContent() ‚Üí HTML ‚Üí texte
  ‚Üì
Return FetchResult {
  html,
  discoveredUrls ‚Üê NOUVEAU
}
  ‚Üì
Crawler : Ajouter discoveredUrls √† la queue
```

## üìù Notes Importantes

### Backward Compatibility

- ‚úÖ **100% compatible** avec le code existant
- Les sites sans JavaScript continuent de fonctionner normalement
- La d√©couverte est opt-in (activ√©e uniquement si framework d√©tect√©)
- Pas d'impact sur les performances des sites statiques

### Configuration par Framework

Le syst√®me s'adapte automatiquement selon le framework d√©tect√© :

| Framework | S√©lecteurs | Max Clics | Strat√©gie |
|-----------|-----------|-----------|-----------|
| WebDev | `a[onclick*="WD_"]`, `button[onclick*="WD_ACTION_"]` | 15 | Hybrid |
| Livewire | `[wire:click*="navigate"]`, `a[wire:navigate]` | 20 | History |
| React | `nav a`, `a[href^="/"]` | 25 | History |
| Vue | `router-link`, `nav a` | 25 | History |

### Patterns d'Exclusion

Automatiquement exclus pour √©viter des actions non-intentionnelles :
- `/logout`, `/login`, `/admin`
- Boutons de cookies, footer
- Liens externes

## üêõ Bugs Connus

1. **Queue initiale vide** : Si seed_url === base_url, la queue peut √™tre vide au d√©marrage
2. **Logs de debug trop verbeux** : Les logs incluent tous les patterns (√† filtrer)
3. **Timeout robots.txt** : Peut bloquer le crawl si respect_robots_txt=true

## üìö Documentation Associ√©e

- `/docs/LEGAL_REASONING_PROMPTS.md` - Prompts juridiques IRAC
- `/docs/SCALABILITY_INDEXING.md` - Scalabilit√© indexation
- `/scripts/add-iort-source.ts` - Script d'ajout source IORT

## üîó Liens Utiles

- **GitHub Actions** : https://github.com/salmenktata/MonCabinet/actions
- **Dernier Workflow** : https://github.com/salmenktata/MonCabinet/actions/runs/21822943419
- **VPS Logs** : `ssh root@84.247.165.187 'docker logs -f moncabinet-nextjs'`

---

**Rapport g√©n√©r√© le** : 9 f√©vrier 2026
**Version du code** : commit `07c4f9b`
**Statut** : D√©ploy√© en production, tests partiels r√©ussis, diagnostic en cours
