# Session Extraction M√©tadonn√©es - Mode FORCE_REGEX_ONLY

**Date** : 12 f√©vrier 2026
**Dur√©e session** : ~2h
**Statut** : ‚úÖ Op√©rationnel en production

---

## üìä √âtat Actuel

**Source** : 9anoun.tn (ID: `4319d2d1-569c-4107-8f52-d71e2a2e9fe9`)

| M√©trique | Valeur |
|----------|--------|
| Pages crawl√©es | 7,776 |
| Pages organis√©es | **40** |
| Pages restantes | 7,736 |
| Couverture | **0.51%** |
| Mode extraction | `FORCE_REGEX_ONLY=true` |

---

## üîß Probl√®me R√©solu

### Sympt√¥me Initial
- Endpoint `/api/cron/extract-metadata` retournait des pages HTML (404) au lieu de JSON
- Extraction bloqu√©e √† 28 pages avec erreurs LLM : "Groq: 401 Invalid API Key" + "Ollama: Request timed out"

### Root Cause
1. **Endpoint 404** : Code d√©ploy√© en production ne contenait pas le dernier commit avec l'endpoint
2. **FORCE_REGEX_ONLY non activ√©** : Variable existait dans `.env` mais pas dans `docker-compose.prod.yml` ‚Üí pas charg√©e dans le container

### Solution Appliqu√©e
1. ‚úÖ Ajout de `FORCE_REGEX_ONLY: ${FORCE_REGEX_ONLY:-false}` dans `docker-compose.prod.yml`
2. ‚úÖ Red√©ploiement Docker complet (build ~9 minutes)
3. ‚úÖ Activation du cron d'extraction automatique

---

## ‚öôÔ∏è Configuration Actuelle

### Variables d'environnement (.env)
```bash
FORCE_REGEX_ONLY=true          # Mode Regex uniquement (pas de LLM)
GROQ_API_KEY=<invalide>        # Cl√© expir√©e (401)
OLLAMA_ENABLED=true            # Mais timeout en prod (trop lent)
```

### Cron Job
```bash
# Toutes les 2 minutes
*/2 * * * * /opt/moncabinet/cron-extract-regex-only.sh
```

### Param√®tres extraction
- **Batch size** : 50 pages par ex√©cution
- **Concurrency** : 10 pages en parall√®le
- **Fr√©quence** : Toutes les 2 minutes
- **M√©thode** : Extraction Regex uniquement (rapide mais limit√©e)

---

## üìà Performance Attendue

### Mode FORCE_REGEX_ONLY (Actuel)
- **Vitesse** : ~50-100 pages/min (instantan√©, pas de LLM)
- **Dur√©e totale** : **1-2 heures** pour 7,736 pages restantes
- **Co√ªt** : **0‚Ç¨** (pas d'appels API)
- **Qualit√©** : Limit√©e (dates, num√©ros extraits, mais pas de champs textuels)

### Mode avec Groq (Optimal - Demain)
- **Vitesse** : ~10-20 pages/min
- **Dur√©e totale** : **4-6 heures** pour 7,736 pages
- **Co√ªt** : ~0.40‚Ç¨ (llama-3.3-70b-versatile)
- **Qualit√©** : Compl√®te (19 champs juridiques extraits)

---

## üéØ R√©sultat Extraction Regex-only

### Champs extraits (limit√©s)
- ‚úÖ `decision_date` : via patterns YYYY-MM-DD, DD/MM/YYYY
- ‚úÖ `decision_number` : via patterns N¬∞ XXXX/YYYY
- ‚úÖ `jort_reference` : via pattern JORT n¬∞ XX du YYYY-MM-DD
- ‚úÖ `text_number` : via pattern Loi n¬∞ XXXX-YY
- ‚ùå `tribunal`, `chambre`, `parties` : N√©cessitent LLM (non extraits)
- ‚ùå `abstract`, `keywords` : N√©cessitent LLM (non extraits)
- ‚ùå `document_type` : N√©cessite LLM (mis √† "autre" par d√©faut)

### Confiance
- **Extraction confidence** : 0.5 (50%) si >0 champs extraits, sinon 0.3 (30%)
- **Extraction method** : `regex_only`
- **LLM provider** : `none`

---

## üìä Monitoring en Temps R√©el

### Option 1 : Script monitoring automatique (Recommand√©)
```bash
/tmp/monitor-extraction-regex.sh
```

Affiche toutes les 30 secondes :
- Pages organis√©es / Total
- Progression depuis le d√©but
- Vitesse instantan√©e (pages/min)
- ETA (estimation temps restant)

### Option 2 : Requ√™te SQL manuelle
```bash
ssh root@84.247.165.187 "docker exec qadhya-postgres psql -U moncabinet -d qadhya -c \"
  SELECT
    COUNT(DISTINCT wpsm.web_page_id) as pages_organisees,
    COUNT(*) as total_pages,
    ROUND((COUNT(DISTINCT wpsm.web_page_id)::numeric / COUNT(*)) * 100, 2) as coverage_percent
  FROM web_pages wp
  LEFT JOIN web_page_structured_metadata wpsm ON wp.id = wpsm.web_page_id
  WHERE wp.web_source_id = '4319d2d1-569c-4107-8f52-d71e2a2e9fe9';
\""
```

### Option 3 : Logs cron
```bash
ssh root@84.247.165.187 "tail -f /var/log/cron-extract-regex.log"
```

---

## üîÑ Plan Optimal pour Demain Matin

### Objectif
Passer en mode **extraction compl√®te avec Groq** pour obtenir les 19 champs juridiques complets.

### √âtapes

1. **Obtenir nouvelle cl√© Groq** (gratuit, 5 min)
   ```
   https://console.groq.com/keys
   ```

2. **Activer Groq en production**
   ```bash
   ./scripts/deploy-groq-and-extract.sh <NOUVELLE_CLE_GROQ>
   ```

   Le script fait automatiquement :
   - Configure `GROQ_API_KEY` dans `.env`
   - D√©sactive `FORCE_REGEX_ONLY`
   - Red√©marre le container
   - Teste la connexion Groq
   - Active le cron extraction Groq (toutes les 2 minutes)
   - Lance le premier batch

3. **Monitoring**
   ```bash
   ssh root@84.247.165.187 'tail -f /var/log/cron-metadata-groq.log'
   ```

### R√©sultat Attendu
- **7,736 pages restantes** extraites en **4-6 heures**
- **19 champs juridiques complets** par page
- **Co√ªt total** : ~0.40‚Ç¨
- **Qualit√©** : 85-95% de confiance

---

## ‚úÖ Tests Effectu√©s

### Test 1 : Endpoint fonctionnel
```bash
curl -X POST http://localhost:3000/api/cron/extract-metadata \
  -H "Authorization: Bearer $CRON_SECRET" \
  -H "Content-Type: application/json" \
  -d '{"sourceId":"4319d2d1-569c-4107-8f52-d71e2a2e9fe9","batchSize":10,"concurrency":3}'

# R√©sultat : {"success":true,"processed":10,"failed":0}
```

### Test 2 : Mode FORCE_REGEX_ONLY actif
```bash
docker exec qadhya-nextjs printenv | grep FORCE_REGEX_ONLY

# R√©sultat : FORCE_REGEX_ONLY=true
```

### Test 3 : Cron actif
```bash
crontab -l | grep cron-extract

# R√©sultat : */2 * * * * /opt/moncabinet/cron-extract-regex-only.sh
```

---

## üìù Fichiers Modifi√©s

### 1. `docker-compose.prod.yml`
```yaml
# Ligne 142 ajout√©e
FORCE_REGEX_ONLY: ${FORCE_REGEX_ONLY:-false}
```

### 2. `/opt/moncabinet/.env` (Production)
```bash
FORCE_REGEX_ONLY=true  # Activ√©
```

### 3. `/opt/moncabinet/cron-extract-regex-only.sh` (Nouveau)
Script cron optimis√© pour mode Regex-only (batch 50, concurrency 10)

---

## üéâ R√©sum√© de la Session

### Avant
- ‚ùå Endpoint 404
- ‚ùå FORCE_REGEX_ONLY non fonctionnel
- üêå 28 pages organis√©es (0.36%)
- ‚è∏Ô∏è  Extraction bloqu√©e (erreurs LLM)

### Apr√®s
- ‚úÖ Endpoint op√©rationnel
- ‚úÖ FORCE_REGEX_ONLY activ√© et fonctionnel
- üöÄ 40 pages organis√©es (0.51%)
- ‚ö° Extraction en cours (cron actif, ~50 pages/min attendu)

### Prochaine √âtape
**Demain matin** : Obtenir nouvelle cl√© Groq ‚Üí Extraction compl√®te 19 champs ‚Üí 7,736 pages en 4-6h

---

## üìû Commandes Utiles

### Arr√™ter le cron
```bash
ssh root@84.247.165.187 "crontab -l | grep -v 'cron-extract' | crontab -"
```

### Relancer manuellement un batch
```bash
ssh root@84.247.165.187 '/opt/moncabinet/cron-extract-regex-only.sh'
```

### V√©rifier logs d'erreur
```bash
ssh root@84.247.165.187 "docker logs qadhya-nextjs --tail 50 | grep 'Metadata Extraction'"
```

### Statistiques d√©taill√©es
```bash
ssh root@84.247.165.187 "docker exec qadhya-postgres psql -U moncabinet -d qadhya -c \"
  SELECT
    extraction_method,
    COUNT(*) as nb_pages,
    AVG(extraction_confidence) as avg_confidence
  FROM web_page_structured_metadata wpsm
  JOIN web_pages wp ON wpsm.web_page_id = wp.id
  WHERE wp.web_source_id = '4319d2d1-569c-4107-8f52-d71e2a2e9fe9'
  GROUP BY extraction_method;
\""
```

---

**D√©ploy√© en production** : https://qadhya.tn
**Logs complets** : `/var/log/cron-extract-regex.log`
**Monitoring dashboard** : https://qadhya.tn/super-admin/web-sources/4319d2d1-569c-4107-8f52-d71e2a2e9fe9
