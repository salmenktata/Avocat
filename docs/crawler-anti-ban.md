# Protection Anti-Bannissement du Crawler Web

## ðŸ“‹ Vue d'ensemble

Le systÃ¨me de protection anti-bannissement protÃ¨ge le crawler web contre les blocages par les sites sources en production. Il combine plusieurs stratÃ©gies:

- âœ… **Retry automatique** avec exponential backoff (429, 503, timeout)
- âœ… **DÃ©tection intelligente de bannissement** (captcha, messages de blocage)
- âœ… **Rate limiting randomisÃ©** pour Ã©viter les patterns de bot
- âœ… **Mode stealth optionnel** avec User-Agents rÃ©alistes
- âœ… **Monitoring complet** avec mÃ©triques et alertes

## ðŸš€ Configuration

### 1. Migration de la base de donnÃ©es

ExÃ©cutez la migration pour ajouter les nouveaux champs:

```bash
psql -U qadhya -d qadhya_db -f db/migrations/20260208_add_anti_ban_fields.sql
```

### 2. Configuration par source

Dans l'interface super-admin, configurez chaque source web:

#### Mode Stealth
- **Par dÃ©faut:** Bot dÃ©clarÃ© `QadhyaBot/1.0` (recommandÃ©)
- **Mode stealth:** User-Agents rÃ©alistes (Chrome, Firefox, Safari)
- **Quand l'activer:** Uniquement si le site bloque le bot de maniÃ¨re injuste

```sql
UPDATE web_sources
SET stealth_mode = TRUE
WHERE id = 'uuid-de-la-source';
```

#### Quotas de crawl
Limitez le nombre de pages crawlÃ©es par pÃ©riode:

```sql
UPDATE web_sources
SET
  max_pages_per_hour = 150,  -- Max 150 pages/heure
  max_pages_per_day = 1500    -- Max 1500 pages/jour
WHERE id = 'uuid-de-la-source';
```

### 3. Configuration globale

Les valeurs par dÃ©faut dans le code:

```typescript
const DEFAULT_ANTI_BAN_CONFIG = {
  // Rate limiting
  baseRateLimitMs: 1500,      // 1.5s entre requÃªtes
  rateLimitVariance: 0.2,      // Â±20% randomisation
  longPauseProbability: 0.05,  // 5% de pauses longues
  longPauseMs: 5000,           // 5 secondes

  // Retry
  maxRetries: 3,
  initialRetryDelayMs: 1000,
  maxRetryDelayMs: 30000,

  // DÃ©tection bannissement
  autoPauseOnBan: true,
  banRetryDelayMs: 3600000,    // 1 heure
}
```

## ðŸ“Š Monitoring

### Dashboard de santÃ©

Consultez le dashboard super-admin pour voir:

- **Taux de succÃ¨s** par source (%)
- **Erreurs HTTP** (429, 403, 503, 5xx)
- **Bannissements dÃ©tectÃ©s**
- **Temps de rÃ©ponse** (moyenne, mÃ©diane, p95)
- **Quotas** (pages crawlÃ©es vs limites)

### MÃ©triques disponibles

```typescript
import { getCrawlerHealthStats } from '@/lib/web-scraper/monitoring-service'

const stats = await getCrawlerHealthStats('source-id', 24) // derniÃ¨res 24h

console.log(`Taux de succÃ¨s: ${stats.successRate}%`)
console.log(`Erreurs 429: ${stats.errors429}`)
console.log(`Bannissements: ${stats.banDetections}`)
```

### VÃ©rifier le statut de bannissement

```typescript
import { getSourceBanStatus } from '@/lib/web-scraper/monitoring-service'

const banStatus = await getSourceBanStatus('source-id')

if (banStatus?.isBanned) {
  console.log(`Source bannie: ${banStatus.reason}`)
  console.log(`Retry aprÃ¨s: ${banStatus.retryAfter}`)
}
```

## ðŸ”§ Utilisation

### Crawl automatique

Le systÃ¨me s'active automatiquement lors du crawl:

```typescript
import { crawlSource } from '@/lib/web-scraper/crawler-service'

const result = await crawlSource(source, {
  maxPages: 100,
  downloadFiles: true,
})

if (result.success) {
  console.log(`Crawl terminÃ©: ${result.pagesProcessed} pages`)
} else {
  console.error(`Ã‰chec: ${result.errors.length} erreurs`)
}
```

### DÃ©bannir manuellement une source

Si une source a Ã©tÃ© bannie par erreur:

```typescript
import { unbanSource } from '@/lib/web-scraper/monitoring-service'

await unbanSource('source-id')
```

Ou via SQL:

```sql
SELECT unban_source('uuid-de-la-source');
```

## ðŸŽ¯ DÃ©tection de bannissement

Le systÃ¨me dÃ©tecte automatiquement:

### 1. Status codes HTTP
- **403 Forbidden** â†’ Confiance haute
- **429 Too Many Requests** â†’ Retry automatique

### 2. Captchas
- Cloudflare (`cf-captcha-container`)
- Google reCAPTCHA (`g-recaptcha`)
- hCaptcha (`h-captcha`)

### 3. Messages de blocage
- "Access Denied"
- "You have been blocked"
- "Rate limit exceeded"
- "Suspicious activity"

### 4. Redirections suspectes
- `/blocked`
- `/captcha`
- `/access-denied`

## ðŸ”„ Retry avec Exponential Backoff

Tentatives automatiques sur erreurs temporaires:

| Tentative | DÃ©lai      | Status codes retryables |
|-----------|------------|-------------------------|
| 1         | 1s         | 429, 503, 504, 408      |
| 2         | 2s Â± 20%   | + timeout, ECONNRESET   |
| 3         | 4s Â± 20%   |                         |
| 4         | 8s Â± 20%   |                         |
| Max       | 30s        |                         |

Le jitter (Â±20%) Ã©vite le "thundering herd" (toutes les requÃªtes en mÃªme temps).

## ðŸ“ˆ MÃ©triques trackÃ©es

Pour chaque source, par heure:

- `total_requests` - Nombre total de requÃªtes
- `successful_requests` - RequÃªtes rÃ©ussies
- `failed_requests` - RequÃªtes Ã©chouÃ©es
- `success_rate` - Taux de succÃ¨s (%)
- `errors_429` - Erreurs "Too Many Requests"
- `errors_403` - Erreurs "Forbidden"
- `errors_503` - Erreurs "Service Unavailable"
- `errors_5xx` - Autres erreurs serveur
- `ban_detections` - Bannissements dÃ©tectÃ©s
- `avg_response_time_ms` - Temps de rÃ©ponse moyen
- `pages_this_hour` - Pages crawlÃ©es cette heure
- `pages_this_day` - Pages crawlÃ©es aujourd'hui

## ðŸ›¡ï¸ Bonnes pratiques

### 1. Commencer conservateur

Pour une nouvelle source:

```sql
UPDATE web_sources
SET
  rate_limit_ms = 2000,        -- 2 secondes
  max_pages_per_hour = 100,
  max_pages_per_day = 1000,
  stealth_mode = FALSE         -- Bot dÃ©clarÃ©
WHERE id = 'nouvelle-source';
```

### 2. Monitorer pendant 24-48h

- VÃ©rifier le taux de succÃ¨s (>95% = bon)
- Surveiller les erreurs 429/403
- Ajuster si nÃ©cessaire

### 3. Augmenter progressivement

Si aucun problÃ¨me aprÃ¨s 48h:

```sql
UPDATE web_sources
SET
  rate_limit_ms = 1500,        -- RÃ©duire Ã  1.5s
  max_pages_per_hour = 150
WHERE id = 'source-stable';
```

### 4. Activer stealth en dernier recours

Uniquement si le site bloque le bot de maniÃ¨re injuste:

```sql
UPDATE web_sources
SET stealth_mode = TRUE
WHERE id = 'source-problematique';
```

## ðŸš¨ Alertes

### Email/Slack (Ã  configurer)

Le systÃ¨me peut envoyer des alertes automatiques:

- Bannissement dÃ©tectÃ© (confiance haute)
- Taux d'erreur > 10% sur 1h
- Quota dÃ©passÃ©
- Source inactive depuis 24h

Configuration dans `lib/web-scraper/monitoring-service.ts` (TODO Phase 3).

## ðŸ“ Logs

Les logs incluent maintenant:

```
[Crawler] DÃ©marrage crawl 9anoun.tn
[Crawler] Rate limit: 1500ms, Max pages: 100, Max depth: 3
[Crawler] Pause longue: 5234ms
[Crawler] Retry 1/3 pour https://9anoun.tn/page dans 1023ms (erreur: timeout)
[Crawler] ðŸš¨ BANNISSEMENT DÃ‰TECTÃ‰ pour 9anoun.tn: Captcha dÃ©tectÃ©
[Crawler] INTERROMPU (bannissement dÃ©tectÃ©)
```

## ðŸ§ª Tests

ExÃ©cutez les tests unitaires:

```bash
npm test lib/web-scraper/__tests__/retry-utils.test.ts
npm test lib/web-scraper/__tests__/anti-ban-utils.test.ts
```

## ðŸ“Š Exemple de requÃªtes SQL utiles

### Sources avec taux d'erreur Ã©levÃ©

```sql
SELECT
  ws.name,
  chm.success_rate,
  chm.errors_429 + chm.errors_403 + chm.errors_503 as total_errors,
  chm.ban_detections
FROM web_sources ws
JOIN crawler_health_metrics chm ON ws.id = chm.web_source_id
WHERE chm.period_start >= NOW() - INTERVAL '24 hours'
  AND chm.success_rate < 90
ORDER BY chm.success_rate ASC;
```

### Sources bannies

```sql
SELECT
  ws.name,
  bs.reason,
  bs.banned_at,
  bs.retry_after,
  bs.detection_confidence
FROM web_sources ws
JOIN web_source_ban_status bs ON ws.id = bs.web_source_id
WHERE bs.is_banned = TRUE;
```

### Nettoyage anciennes mÃ©triques

```sql
-- Garder 30 jours d'historique
DELETE FROM crawler_health_metrics
WHERE period_start < NOW() - INTERVAL '30 days';
```

## ðŸ”® AmÃ©liorations futures

- [ ] Dashboard temps rÃ©el (WebSocket)
- [ ] Alertes Email/Slack automatiques
- [ ] Rotation d'IP via proxies (si budget)
- [ ] ML pour dÃ©tecter patterns de bannissement
- [ ] Auto-ajustement du rate limit basÃ© sur les erreurs
