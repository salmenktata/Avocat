#\!/bin/bash
#
# Cron automatique : RÃ©analyse des documents KB Ã©chouÃ©s
#
# Schedule recommandÃ© : Quotidien 4h du matin (aprÃ¨s indexation overnight)
# Crontab : 0 4 * * * /opt/qadhya/scripts/cron-reanalyze-kb-failures.sh
#
# Logs : /var/log/qadhya/reanalyze-kb.log
#

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Charger library cron logging
source "$SCRIPT_DIR/lib/cron-logger.sh"

LOG_DIR="/var/log/qadhya"
LOG_FILE="${LOG_DIR}/reanalyze-kb.log"

# Utiliser CRON_API_BASE depuis env (injectÃ© par trigger server) ou dÃ©faut
CRON_API_BASE="${CRON_API_BASE:-https://qadhya.tn}"
API_URL="${CRON_API_BASE}/api/admin/kb/reanalyze-failed"

BATCH_SIZE=50
MAX_BATCHES=5  # Maximum 5 batches = 250 docs/jour

# CrÃ©er rÃ©pertoire logs si nÃ©cessaire
mkdir -p "$LOG_DIR"

# Fonction de logging
log() {
  echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

log "=========================================="
log "DÃ©but rÃ©analyse automatique KB Ã©checs"
log "=========================================="

# DÃ©tecter noms des conteneurs (robuste contre redÃ©marrages)
NEXTJS_CONTAINER=$(docker ps --filter "name=nextjs" --format "{{.Names}}" | head -1)
POSTGRES_CONTAINER=$(docker ps --filter "name=postgres" --format "{{.Names}}" | head -1)

if [ -z "$NEXTJS_CONTAINER" ] || [ -z "$POSTGRES_CONTAINER" ]; then
  log "âŒ ERREUR: Conteneurs Docker non trouvÃ©s"
  log "   Next.js: $NEXTJS_CONTAINER"
  log "   PostgreSQL: $POSTGRES_CONTAINER"

  cron_fail "Conteneurs Docker non trouvÃ©s" 1
  exit 1
fi

log "ğŸ³ Conteneurs dÃ©tectÃ©s:"
log "   Next.js: $NEXTJS_CONTAINER"
log "   PostgreSQL: $POSTGRES_CONTAINER"

# RÃ©cupÃ©rer CRON_SECRET (prioritÃ©: env var > docker exec)
if [ -z "$CRON_SECRET" ]; then
  log "ğŸ”‘ RÃ©cupÃ©ration CRON_SECRET depuis container..."
  if \! CRON_SECRET=$(docker exec "$NEXTJS_CONTAINER" env | grep CRON_SECRET | cut -d= -f2); then
    log "âŒ ERREUR: Impossible de rÃ©cupÃ©rer CRON_SECRET"
    cron_fail "Impossible de rÃ©cupÃ©rer CRON_SECRET" 1
    exit 1
  fi

  if [ -z "$CRON_SECRET" ]; then
    log "âŒ ERREUR: CRON_SECRET vide"
    cron_fail "CRON_SECRET vide" 1
    exit 1
  fi
else
  log "âœ… CRON_SECRET trouvÃ© en environnement"
fi

export CRON_SECRET

# DÃ©marrer tracking cron
cron_start "reanalyze-kb-failures" "scheduled"

# Fonction trap pour cleanup
cleanup() {
  local exit_code=$?
  if [ $exit_code -ne 0 ]; then
    log "âŒ Script terminÃ© avec erreur (exit $exit_code)"
    cron_fail "Script terminated with error" $exit_code
  fi
}

trap cleanup EXIT

# Compter Ã©checs initiaux
log "ğŸ“Š Comptage documents Ã©chouÃ©s..."
TOTAL_FAILURES=$(docker exec "$POSTGRES_CONTAINER" psql -U moncabinet -d qadhya -t -c \
  "SELECT COUNT(*) FROM knowledge_base WHERE quality_score = 50 AND is_active = true;")
TOTAL_FAILURES=$(echo "$TOTAL_FAILURES" | tr -d ' ')

log "ğŸ”´ Total documents Ã©chouÃ©s: $TOTAL_FAILURES"

if [ "$TOTAL_FAILURES" -eq 0 ]; then
  log "âœ… Aucun document Ã©chouÃ© Ã  rÃ©analyser"
  trap - EXIT
  OUTPUT_JSON="{\"totalFailures\": 0, \"batchesProcessed\": 0, \"reanalyzed\": 0, \"fixed\": 0}"
  cron_complete "$OUTPUT_JSON"
  exit 0
fi

# Traitement par batch
BATCH_COUNT=0
TOTAL_REANALYZED=0
TOTAL_FIXED=0

while [ $BATCH_COUNT -lt $MAX_BATCHES ]; do
  BATCH_COUNT=$((BATCH_COUNT + 1))
  log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  log "ğŸ“¦ Batch $BATCH_COUNT / $MAX_BATCHES"
  log "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

  # Appel API rÃ©analyse
  log "ğŸš€ Appel API rÃ©analyse (batch_size=$BATCH_SIZE)..."
  RESPONSE=$(curl -s -X POST "$API_URL" \
    -H "Content-Type: application/json" \
    -H "X-Cron-Secret: $CRON_SECRET" \
    -d "{\"batchSize\": $BATCH_SIZE}" \
    -w "\n%{http_code}")

  HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
  BODY=$(echo "$RESPONSE" | head -n-1)

  if [ "$HTTP_CODE" \!= "200" ]; then
    log "âŒ ERREUR API (HTTP $HTTP_CODE): $BODY"
    trap - EXIT
    cron_fail "API error HTTP $HTTP_CODE" 1
    exit 1
  fi

  # Parser rÃ©sultat
  BATCH_REANALYZED=$(echo "$BODY" | grep -o '"reanalyzed":[0-9]*' | cut -d: -f2)
  BATCH_FIXED=$(echo "$BODY" | grep -o '"fixed":[0-9]*' | cut -d: -f2)

  log "ğŸ“ˆ RÃ©sultat batch:"
  log "   - RÃ©analysÃ©s: $BATCH_REANALYZED"
  log "   - FixÃ©s: $BATCH_FIXED"

  TOTAL_REANALYZED=$((TOTAL_REANALYZED + BATCH_REANALYZED))
  TOTAL_FIXED=$((TOTAL_FIXED + BATCH_FIXED))

  # ArrÃªter si aucun doc rÃ©analysÃ©
  if [ "$BATCH_REANALYZED" -eq 0 ]; then
    log "â„¹ï¸  Aucun document restant, arrÃªt"
    break
  fi

  # Pause entre batches (Ã©viter surcharge OpenAI)
  if [ $BATCH_COUNT -lt $MAX_BATCHES ]; then
    log "â¸ï¸  Pause 5s avant prochain batch..."
    sleep 5
  fi
done

# Compter Ã©checs restants
REMAINING_FAILURES=$(docker exec "$POSTGRES_CONTAINER" psql -U moncabinet -d qadhya -t -c \
  "SELECT COUNT(*) FROM knowledge_base WHERE quality_score = 50 AND is_active = true;")
REMAINING_FAILURES=$(echo "$REMAINING_FAILURES" | tr -d ' ')

log "=========================================="
log "âœ… RÃ©analyse terminÃ©e"
log "=========================================="
log "ğŸ“Š RÃ©sumÃ©:"
log "   - Ã‰checs initiaux: $TOTAL_FAILURES"
log "   - Batches traitÃ©s: $BATCH_COUNT"
log "   - Documents rÃ©analysÃ©s: $TOTAL_REANALYZED"
log "   - Documents fixÃ©s: $TOTAL_FIXED"
log "   - Ã‰checs restants: $REMAINING_FAILURES"
log "=========================================="

# DÃ©sactiver trap avant succÃ¨s
trap - EXIT

# ComplÃ©ter avec succÃ¨s
OUTPUT_JSON="{\"totalFailures\": $TOTAL_FAILURES, \"batchesProcessed\": $BATCH_COUNT, \"reanalyzed\": $TOTAL_REANALYZED, \"fixed\": $TOTAL_FIXED, \"remaining\": $REMAINING_FAILURES}"
cron_complete "$OUTPUT_JSON"

log "ğŸ‰ Script terminÃ© avec succÃ¨s"
exit 0
